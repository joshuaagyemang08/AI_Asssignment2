# -*- coding: utf-8 -*-
"""Joshua_Agyemang._SportsPrediction.pynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uGNffq0oHl9GN4AhnbKv1yRGjoDHTh1c
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.preprocessing import MinMaxScaler
import joblib
import xgboost as xgb

# Specify the file path or URL
train_file_path = '/content/drive/MyDrive/Colab Notebooks/data/male_players (legacy).csv'
test_file_path = '/content/drive/MyDrive/Colab Notebooks/data/players_22.csv'

# Read the CSV file
df_train = pd.read_csv(train_file_path)

# Display the first few rows of the DataFrame
print(df_train.head())

df_train.shape

df_train.describe()

df_train.info()

# Print each column name on a new line
for column in df_train.columns:
    print(column)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
df_train.hist(bins=50, figsize=(20,15))
plt.show()

df_test = pd.read_csv(test_file_path)

# Display the first few rows of the DataFrame
print(df_test.head())

df_test.shape

df_test.describe()

df_test.info()

# Print each column name on a new line
for column in df_test.columns:
    print(column)

train_irrelevant_columns = [
    'player_id', 'player_url', 'fifa_version', 'fifa_update', 'fifa_update_date',
    'club_team_id', 'club_name', 'club_position', 'club_jersey_number', 'club_loaned_from',
    'club_joined_date', 'club_contract_valid_until_year', 'nationality_id', 'nationality_name',
    'nation_team_id', 'nation_position', 'nation_jersey_number', 'player_face_url'
]
train_irrelevant_columns = [col for col in train_irrelevant_columns if col in df_train.columns]
df_train.drop(columns= train_irrelevant_columns, inplace=True)

test_irrelevant_columns = [
    'sofifa_id','player_url','club_team_id', 'club_name', 'club_position', 'club_jersey_number', 'club_loaned_from', 'nationality_id', 'nationality_name',
    'nation_team_id', 'nation_position', 'nation_jersey_number', 'player_face_url'
]
test_irrelevant_columns = [col for col in test_irrelevant_columns if col in df_test.columns]
df_test.drop(columns= test_irrelevant_columns, inplace=True)

df_train

df_test

df_train.iloc[25:51]

selected_columns = [
    'short_name', 'long_name', 'league_level', 'weak_foot', 'skill_moves', 'international_reputation',
    'pace', 'shooting', 'passing', 'dribbling', 'defending', 'physic',
    'attacking_crossing', 'attacking_finishing', 'attacking_heading_accuracy',
    'attacking_short_passing', 'attacking_volleys', 'skill_dribbling', 'skill_curve',
    'skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control',
    'movement_acceleration', 'movement_sprint_speed', 'movement_agility',
    'movement_reactions', 'movement_balance', 'power_shot_power', 'power_jumping',
    'power_stamina', 'power_strength', 'power_long_shots', 'mentality_aggression',
    'mentality_interceptions', 'mentality_positioning', 'mentality_vision',
    'mentality_penalties', 'mentality_composure', 'defending_marking_awareness',
    'defending_standing_tackle', 'defending_sliding_tackle', 'value_eur',
    'wage_eur', 'release_clause_eur', 'goalkeeping_diving',
    'goalkeeping_handling', 'goalkeeping_kicking', 'goalkeeping_positioning',
    'goalkeeping_reflexes', 'goalkeeping_speed', 'age','overall'
]

new_df_train = df_train[selected_columns]

new_df_train

Less_than_threshold = []
More_than_thresshold = []

for i in new_df_train.columns:
    if((new_df_train[i].isnull().sum())<(0.4*(new_df_train.shape[0]))):
        Less_than_threshold.append(i)

    else:
       More_than_thresshold.append(i)

new_df_train = df_train[Less_than_threshold]

print(Less_than_threshold)

print(More_than_thresshold)

new_df_train.isnull().sum()

mean = new_df_train['league_level'].mean()
new_df_train['league_level'].fillna(mean, inplace = True)

mean = new_df_train['pace'].mean()
new_df_train['pace'].fillna(mean, inplace = True)

mean = new_df_train['shooting'].mean()
new_df_train['shooting'].fillna(mean, inplace = True)

mean = new_df_train['passing'].mean()
new_df_train['passing'].fillna(mean, inplace = True)

mean = new_df_train['dribbling'].mean()
new_df_train['dribbling'].fillna(mean, inplace = True)

mean = new_df_train['defending'].mean()
new_df_train['defending'].fillna(mean, inplace = True)

mean = new_df_train['physic'].mean()
new_df_train['physic'].fillna(mean, inplace = True)

mean = new_df_train['mentality_composure'].mean()
new_df_train['mentality_composure'].fillna(mean, inplace = True)

mean = new_df_train['value_eur'].mean()
new_df_train['value_eur'].fillna(mean, inplace = True)

mean = new_df_train['wage_eur'].mean()
new_df_train['wage_eur'].fillna(mean, inplace = True)

mean = new_df_train['release_clause_eur'].mean()
new_df_train['release_clause_eur'].fillna(mean, inplace = True)

new_df_train.head()

new_df_train.tail()

new_df_train.columns.tolist()

print(new_df_train.isnull().sum(), "\n\n")

# Identifying categorical and numerical columns
categorical_cols = new_df_train.select_dtypes(include=['object']).columns
numerical_cols = new_df_train.select_dtypes(include=['number']).columns

numerical_cols = numerical_cols.drop('overall')

# Encoding categorical variables
label_encoder = LabelEncoder()
for col in categorical_cols:
    new_df_train[col] = label_encoder.fit_transform(new_df_train[col].astype(str))

# Scaling numerical variables
scaler = StandardScaler()
new_df_train[numerical_cols] = scaler.fit_transform(new_df_train[numerical_cols])

# Display the processed DataFrame
print(new_df_train.head())

# About to drop the dependant variables to be able to create a new data frame that contains the only the independent variables
independent_variables = new_df_train.drop(columns=['overall'])

#extracting the target variable so that we can run a correlation study on the independent
#variables and the overall rating to find out which has the greatest correlation.
dependent_variable = new_df_train['overall']
correlation_matrix = independent_variables.corrwith(dependent_variable)
print(correlation_matrix)

#Sort the correlation in descending order from the correlation matrix to identify features with the highest correlations.

sorted_correlations = correlation_matrix.abs().sort_values(ascending=False)

sorted_correlations

#selecting some featres with the highest correlation

n_top_features = 15
top_features = sorted_correlations[:n_top_features]

# Visualize the top correlated features
plt.figure(figsize=(12, 8))
sns.barplot(x=top_features, y=top_features.index, palette="Blues_d")
plt.title("Top Correlated Features with Overall Rating")
plt.xlabel("Correlation")
plt.ylabel("Feature")
plt.show()

#creating a feature subset using the top correlated features
feature_subsets = new_df_train[top_features.index]

#scaling the independent variables
scaler = StandardScaler()
scaled_features = scaler.fit_transform(feature_subsets)

#creating a DataFrame with the scaled features
scaled_df = pd.DataFrame(scaled_features, columns=feature_subsets.columns)

scaled_df.reset_index(drop=True, inplace=True)
dependent_variable.reset_index(drop=True, inplace=True)

#concatenating the scaled features with the dependent variable
final_data = pd.concat([scaled_df, dependent_variable],axis = 1)

final_data

final_data.columns.tolist()

final_data

X = final_data.drop(columns=['overall'])
y = final_data['overall']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

n_folds = 10
kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)

#Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_scores = cross_val_score(rf_model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')
rf_rmse = np.sqrt(-rf_scores.mean())

rf_model.fit(X_train, y_train)
score = rf_model.score(X_test, y_test)
mse = mean_squared_error(y_test, rf_model.predict(X_test))

print("RMSE: %.4f" % mse)
print("size of prediction: ", len(rf_model.predict(X_test)))
print("prediction: \n", rf_model.predict(X_test))
print("test score: {0:.4f}\n".format(score))

#XGBoost Regressor
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
xgb_scores = cross_val_score(xgb_model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')
xgb_rmse = np.sqrt(-xgb_scores.mean())
xgb_model.fit(X_train, y_train)
score = xgb_model.score(X_test, y_test)
mse = mean_squared_error(y_test, xgb_model.predict(X_test))

print("RMSE: %.4f" % np.sqrt(mse))
print("size of prediction: ", len(xgb_model.predict(X_test)))
print("prediction: \n", xgb_model.predict(X_test))
print("test score: {0:.4f}\n".format(score))

#Gradient Boosting Regressor
gb_model = GradientBoostingRegressor(n_estimators=500, random_state=42, max_depth=4, min_samples_split=2, learning_rate=0.01)
gb_scores = cross_val_score(gb_model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')
gb_rmse = np.sqrt(-gb_scores.mean())
gb_model.fit(X_train, y_train)
score = gb_model.score(X_test, y_test)
mse = mean_squared_error(y_test, gb_model.predict(X_test))

print("RMSE: %.4f" % mse)
print("size of prediction: ", len(gb_model.predict(X_test)))
print("prediction: \n", gb_model.predict(X_test))
print("test score: {0:.4f}\n".format(score))

"""Due to the risk of overfitting the models, I opted to only finetune the models with a mean squared error above 0.1. As such, I only optimized the XGB model."""

params = {'n_estimators': 1000, 'max_depth': 4, 'min_samples_split': 2,
          'learning_rate': 0.1, 'loss': 'squared_error'}
improved_gbr = GradientBoostingRegressor(**params)

improved_gbr.fit(X_train, y_train)

score = improved_gbr.score(X_test, y_test)

# calculate the Mean Squared Error
mse = mean_squared_error(y_test, improved_gbr.predict(X_test))

print("MSE: %.4f" % mse)
print("size of prediction: ", len(improved_gbr.predict(X_test)))
print("prediction: \n", improved_gbr.predict(X_test))
print("test score: {0:.4f}\n".format(score))

params = {'n_estimators': 10000, 'max_depth': 4, 'learning_rate': 0.01, 'subsample':0.8}

improved_xgb = xgb.XGBRegressor(**params)

improved_xgb.fit(X_train, y_train)

score = improved_xgb.score(X_test, y_test)

# calculate the Mean Squared Error
mse = mean_squared_error(y_test, improved_xgb.predict(X_test))

print("MSE: %.4f" % mse)
print("size of prediction: ", len(improved_xgb.predict(X_test)))
print("prediction: \n", improved_xgb.predict(X_test))
print("test score: {0:.4f}\n".format(score))

ensemble = VotingRegressor(estimators=[
    ('gb_model', gb_model),
    ('improved_xgb', improved_xgb),
    ('rf_model', rf_model),
])

ensemble.fit(X_train, y_train)
# Make predictions using the ensemble modelThis VotingRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.This VotingRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
y_pred = ensemble.predict(X_test)
score = ensemble.score(X_test, y_test)


# Calculate RMSE (Root Mean Squared Error)
mse = mean_squared_error(y_test, y_pred)

print("MSE: %.4f" % mse)
print("size of prediction: ", len(ensemble.predict(X_test)))
print("prediction: \n", ensemble.predict(X_test))
print("test score: {0:.4f}\n".format(score))

df_test.isnull().sum()

mean = df_test['goalkeeping_speed'].mean()
df_test['goalkeeping_speed'].fillna(mean, inplace = True)

selected_features = [
    'short_name', 'long_name', 'league_level', 'weak_foot', 'skill_moves', 'international_reputation',
    'pace', 'shooting', 'passing', 'dribbling', 'defending', 'physic',
    'attacking_crossing', 'attacking_finishing', 'attacking_heading_accuracy',
    'attacking_short_passing', 'attacking_volleys', 'skill_dribbling', 'skill_curve',
    'skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control',
    'movement_acceleration', 'movement_sprint_speed', 'movement_agility',
    'movement_reactions', 'movement_balance', 'power_shot_power', 'power_jumping',
    'power_stamina', 'power_strength', 'power_long_shots', 'mentality_aggression',
    'mentality_interceptions', 'mentality_positioning', 'mentality_vision',
    'mentality_penalties', 'mentality_composure', 'defending_marking_awareness',
    'defending_standing_tackle', 'defending_sliding_tackle','value_eur',
    'wage_eur', 'release_clause_eur', 'goalkeeping_diving',
    'goalkeeping_handling', 'goalkeeping_kicking', 'goalkeeping_positioning',
    'goalkeeping_reflexes', 'goalkeeping_speed','age','overall'
]

new_test_data = df_test[selected_features]

new_test_data.isnull().sum()

mean = new_test_data['league_level'].mean()
new_test_data['league_level'].fillna(mean, inplace = True)

mean = new_test_data['pace'].mean()
new_test_data['pace'].fillna(mean, inplace = True)

mean = new_test_data['shooting'].mean()
new_test_data['shooting'].fillna(mean, inplace = True)

mean = new_test_data['passing'].mean()
new_test_data['passing'].fillna(mean, inplace = True)

mean = new_test_data['dribbling'].mean()
new_test_data['dribbling'].fillna(mean, inplace = True)

mean = new_test_data['defending'].mean()
new_test_data['defending'].fillna(mean, inplace = True)

mean = new_test_data['physic'].mean()
new_test_data['physic'].fillna(mean, inplace = True)

mean = new_test_data['value_eur'].mean()
new_test_data['value_eur'].fillna(mean, inplace = True)

mean = new_test_data['wage_eur'].mean()
new_test_data['wage_eur'].fillna(mean, inplace = True)

mean = new_test_data['release_clause_eur'].mean()
new_test_data['release_clause_eur'].fillna(mean, inplace = True)

print(new_test_data.isnull().sum(), "\n\n")

subsets = new_test_data[['movement_reactions',
 'passing',
 'wage_eur',
 'mentality_composure',
 'value_eur',
 'dribbling',
 'attacking_short_passing',
 'mentality_vision',
 'international_reputation',
 'skill_long_passing',
 'power_shot_power',
 'physic',
 'release_clause_eur',
 'age',
 'skill_ball_control',
                         'overall']]


#New
# Identifying categorical and numerical columns
categorical_cols = subsets.select_dtypes(include=['object']).columns
numerical_cols = subsets.select_dtypes(include=['number']).columns

numerical_cols = numerical_cols.drop('overall')


# Encoding categorical variables
label_encoder = LabelEncoder()
for col in categorical_cols:
    subsets[col] = label_encoder.fit_transform(subsets[col].astype(str))

# Scaling numerical variables
scaler = StandardScaler()
subsets[numerical_cols] = scaler.fit_transform(subsets[numerical_cols])

# Display the processed DataFrame
print(subsets.head())

t_scaler = StandardScaler()
t_scaled_features = t_scaler.fit_transform(subsets)

# Create a DataFrame with scaled features
t_scaled_df = pd.DataFrame(t_scaled_features, columns=subsets.columns)

t_scaled_df.reset_index(drop=True, inplace=True)


dependent_variable = subsets['overall']

t_scaled_df.drop(columns=['overall'], inplace=True)

dependent_variable.reset_index(drop=True, inplace=True)

# # Combine scaled features with the dependent variable
t_final_data = pd.concat([t_scaled_df, dependent_variable],axis = 1)
t_final_data

t_final_data

X_22 = t_final_data.drop(columns=['overall'])
y_22 = t_final_data['overall']

X_22

y_22

score = ensemble.score(X_22, y_22)

# Make predictions using the ensemble model
y_pred_22 = ensemble.predict(X_22)

# Calculate RMSE (Root Mean Squared Error)
mse = mean_squared_error(y_22, y_pred_22)

print("RMSE: %.4f" % np.sqrt(mse))
print("size of prediction: ", len(ensemble.predict(X_22)))
print("prediction: \n", ensemble.predict(X_22))
print("test score: {0:.4f}\n".format(score))

score = gb_model.score(X_22, y_22)


# Make predictions using the ensemble model
y_pred_22 = gb_model.predict(X_22)

# Calculate RMSE (Root Mean Squared Error)
mse = mean_squared_error(y_22, y_pred_22)

print("MSE: %.4f" % mse)
print("size of prediction: ", len(gb_model.predict(X_22)))
print("prediction: \n", gb_model.predict(X_22))
print("test score: {0:.4f}\n".format(score))

score = rf_model.score(X_22, y_22)


# Make predictions using the ensemble model
y_pred_22 = rf_model.predict(X_22)

# Calculate RMSE (Root Mean Squared Error)
mse = mean_squared_error(y_22, y_pred_22)

print("MSE: %.4f" % mse)
print("size of prediction: ", len(rf_model.predict(X_22)))
print("prediction: \n", rf_model.predict(X_22))
print("test score: {0:.4f}\n".format(score))

score = improved_xgb.score(X_22, y_22)


# Make predictions using the ensemble model
y_pred_22 = improved_xgb.predict(X_22)

# Calculate RMSE (Root Mean Squared Error)
mse = mean_squared_error(y_22, y_pred_22)

print("MSE: %.4f" % mse)
print("size of prediction: ", len(improved_xgb.predict(X_22)))
print("prediction: \n", improved_xgb.predict(X_22))
print("test score: {0:.4f}\n".format(score))

joblib.dump(ensemble,'/content/drive/My Drive/Colab Notebooks/final_model.pkl')

!pip show scikit-learn

!pip freeze '/content/drive/My Drive/Colab Notebooks/requirements.txt'